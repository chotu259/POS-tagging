{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnpH2aGx5Azp",
        "outputId": "af13dcf4-be4a-4053-b3ae-9a59836881a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0 (from gradio)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1.0->gradio)\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-4.44.0-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading ruff-0.6.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, h11, ffmpy, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.0 ffmpy-0.4.0 gradio-4.44.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.12 ruff-0.6.8 semantic-version-2.10.0 starlette-0.38.6 tomlkit-0.12.0 uvicorn-0.31.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')  # Optional, if tokenizing\n",
        "nltk.download('universal_tagset')  # For Universal POS tagging\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "#import numpy as np\n",
        "#import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#mport seaborn as sns\n",
        "#mport matplotlib.pyplot as plt\n",
        "import gradio as gr\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zic3sPOl_KK8",
        "outputId": "af0141f3-3b2c-444e-ec90-0be2c66cf16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class POSTagger:\n",
        "\n",
        "    def __init__(self, train_data, test_data, tagset) -> None:\n",
        "\n",
        "        self.test_data = test_data\n",
        "        self.train_data = train_data\n",
        "        self.tagset = tagset\n",
        "        self.tag_count = self.getTagCount()\n",
        "        self.lexical_prob = self.lexical_probability()\n",
        "        self.transition_prob = self.transition_probability()\n",
        "        self.start_prob = self.start_probability()\n",
        "        self.end_prob = self.end_probability()\n",
        "\n",
        "    def getTagCount(self):\n",
        "        tag_count = defaultdict(int)\n",
        "        for sent in self.train_data:\n",
        "            for (w, t) in sent:\n",
        "                tag_count[t] += 1\n",
        "        return tag_count\n",
        "\n",
        "    def lexical_probability(self):\n",
        "        lexical_prob = defaultdict(lambda: defaultdict(float))\n",
        "        for s in self.train_data:\n",
        "            for (w, t) in s:\n",
        "                lexical_prob[w.lower()][t] += 1\n",
        "        return lexical_prob\n",
        "\n",
        "    def transition_probability(self):\n",
        "        transition_prob = defaultdict(lambda: defaultdict(float))\n",
        "        for sent in self.train_data:\n",
        "            for (w1, t1), (w2, t2) in nltk.bigrams(sent):\n",
        "                transition_prob[t2][t1] += 1\n",
        "        for t2 in self.tagset:\n",
        "            for t1 in self.tagset:\n",
        "                transition_prob[t2][t1] /= self.tag_count[t1]\n",
        "        return transition_prob\n",
        "\n",
        "    def start_probability(self):\n",
        "        start_prob = defaultdict(float)\n",
        "        for sent in self.train_data:\n",
        "            start_prob[sent[0][1]] += 1\n",
        "        for t in self.tagset:\n",
        "            start_prob[t] /= len(self.train_data)\n",
        "        return start_prob\n",
        "\n",
        "    def end_probability(self):\n",
        "        end_prob = defaultdict(float)\n",
        "        for sent in self.train_data:\n",
        "            end_prob[sent[len(sent)-1][1]] += 1\n",
        "        for t in self.tagset:\n",
        "            end_prob[t] /= len(self.train_data)\n",
        "        return end_prob\n",
        "\n",
        "    def viterbi(self, words):\n",
        "        dp = [defaultdict(float) for _ in range(len(words) + 1)]\n",
        "        backpointers = [defaultdict(int) for _ in range(len(words)+1)]\n",
        "\n",
        "        prod = 0\n",
        "        for t in self.tagset:\n",
        "            prod += self.lexical_prob[words[0]][t]\n",
        "        if prod == 0:\n",
        "            for t in self.tagset:\n",
        "                dp[0][t] = self.start_prob[t]\n",
        "        else:\n",
        "            for t in self.tagset:\n",
        "                dp[0][t] = self.start_prob[t] * self.lexical_prob[words[0]][t]/self.tag_count[t]\n",
        "\n",
        "        for i in range(1, len(words)):\n",
        "            prod = 0\n",
        "            for t in self.tagset:\n",
        "                prod += self.lexical_prob[words[i]][t]\n",
        "            if prod == 0:\n",
        "                for t in self.tagset:\n",
        "                    dp[i][t], backpointers[i][t] = max((dp[i-1][prev_t] * self.transition_prob[t][prev_t] , prev_t) for prev_t in self.tagset)\n",
        "                continue\n",
        "            for t in self.tagset:\n",
        "                dp[i][t], backpointers[i][t] = max((dp[i-1][prev_t] * self.transition_prob[t][prev_t] * self.lexical_prob[words[i]][t]/self.tag_count[t], prev_t) for prev_t in self.tagset)\n",
        "\n",
        "        dp[len(words)]['.'], backpointers[len(words)]['.'] = max((dp[len(words)-1][prev_t] * self.end_prob[prev_t], prev_t) for prev_t in self.tagset)\n",
        "        best_path = [backpointers[len(words)]['.']]\n",
        "\n",
        "        for i in range(len(words)-1, 0, -1):\n",
        "            best_path.append(backpointers[i][best_path[-1]])\n",
        "\n",
        "        best_path.reverse()\n",
        "\n",
        "        return best_path\n",
        "    def evaluate(self):\n",
        "      true_tags = []\n",
        "      predicted_tags = []\n",
        "\n",
        "      for sentence in self.test_data:\n",
        "        words = [w for w, t in sentence]\n",
        "        true_tags_sentence = [t for w, t in sentence]\n",
        "        predicted_tags_sentence = self.viterbi(words)\n",
        "\n",
        "        true_tags.extend(true_tags_sentence)\n",
        "        predicted_tags.extend(predicted_tags_sentence)\n",
        "\n",
        "    # Calculate overall metrics\n",
        "      overall_precision = precision_score(true_tags, predicted_tags, average='weighted', zero_division=0)\n",
        "      overall_recall = recall_score(true_tags, predicted_tags, average='weighted', zero_division=0)\n",
        "      overall_f1 = f1_score(true_tags, predicted_tags, average='weighted', zero_division=0)\n",
        "\n",
        "    # Confusion matrix\n",
        "      conf_matrix = confusion_matrix(true_tags, predicted_tags, labels=self.tagset)\n",
        "\n",
        "    # Print overall metrics\n",
        "      print(\"Overall Precision:\", overall_precision)\n",
        "      print(\"Overall Recall:\", overall_recall)\n",
        "      print(\"Overall F1 Score:\", overall_f1)\n",
        "      print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "    # Calculate metrics for individual tags\n",
        "      print(\"\\nMetrics for Individual Tags:\")\n",
        "      report = classification_report(true_tags, predicted_tags, labels=self.tagset)\n",
        "      print(report)\n",
        "\n",
        "    # Optionally you can add your test method here (omitted for brevity)\n",
        "\n"
      ],
      "metadata": {
        "id": "Fwxt_LXA7bcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load data and initialize tagger\n",
        "    data = brown.tagged_sents(tagset='universal')\n",
        "    tagset = ['PRT', 'ADP', 'DET', 'ADV', 'PRON', '.', 'NUM', 'NOUN', 'ADJ', 'X', 'VERB', 'CONJ']\n",
        "    tagger = POSTagger(data, data, tagset)\n",
        "    tagger.evaluate()\n",
        "    # Function to predict POS tags for Gradio\n",
        "    def predict(sentence):\n",
        "        words = [word.lower() for word in word_tokenize(sentence)]\n",
        "        pos_tags = tagger.viterbi(words)\n",
        "        return list(zip(words, pos_tags))  # Pair words with their POS tags\n",
        "\n",
        "    # Gradio Interface\n",
        "    interface = gr.Interface(\n",
        "        fn=predict,\n",
        "        inputs=\"text\",\n",
        "        outputs=\"text\",\n",
        "        title=\"POS Tagger\",\n",
        "        description=\"Input a sentence to get POS tags.\"\n",
        "    )\n",
        "    interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n6nafb6N_R4X",
        "outputId": "ce94c041-3ab3-4f2a-c6bb-0dbc016e3a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Precision: 0.9223084474368672\n",
            "Overall Recall: 0.9194594864587424\n",
            "Overall F1 Score: 0.9195131566826688\n",
            "Confusion Matrix:\n",
            " [[ 24153   2349    354    261   1692     24      0    682     43     15\n",
            "     256      0]\n",
            " [  2194 136754   2274   1823    892     72      1    368     72     64\n",
            "     112    140]\n",
            " [     1   1427 132235    224   2657      9      2    357     15     48\n",
            "      30     14]\n",
            " [   633   3152    953  45893    704     43      0   2537   2105     15\n",
            "     124     80]\n",
            " [   197    442    495    366  46142     12      0    813      7      6\n",
            "     854      0]\n",
            " [     0      0      0      0      0 147482      0      0      0     83\n",
            "       0      0]\n",
            " [     0     37    406      8     49     11  13848    493     12      6\n",
            "       4      0]\n",
            " [   278   4761  13094    288   6799   1055    305 238229   5282    163\n",
            "    5303      1]\n",
            " [   180    581   3772   2120    136     38      4   3055  73305     35\n",
            "     495      0]\n",
            " [     3     66    113      2      9     21      0    234     38    867\n",
            "      30      3]\n",
            " [     5   1422    800    224    361    253      0   5549    748     67\n",
            "  173321      0]\n",
            " [     0    978    431    383    326     44      0    474      9     25\n",
            "      41  35440]]\n",
            "\n",
            "Metrics for Individual Tags:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         PRT       0.87      0.81      0.84     29829\n",
            "         ADP       0.90      0.94      0.92    144766\n",
            "         DET       0.85      0.97      0.91    137019\n",
            "         ADV       0.89      0.82      0.85     56239\n",
            "        PRON       0.77      0.94      0.85     49334\n",
            "           .       0.99      1.00      0.99    147565\n",
            "         NUM       0.98      0.93      0.95     14874\n",
            "        NOUN       0.94      0.86      0.90    275558\n",
            "         ADJ       0.90      0.88      0.89     83721\n",
            "           X       0.62      0.63      0.62      1386\n",
            "        VERB       0.96      0.95      0.95    182750\n",
            "        CONJ       0.99      0.93      0.96     38151\n",
            "\n",
            "    accuracy                           0.92   1161192\n",
            "   macro avg       0.89      0.89      0.89   1161192\n",
            "weighted avg       0.92      0.92      0.92   1161192\n",
            "\n",
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://715136b5a434f48a95.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://715136b5a434f48a95.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**this code handles unknown words handling(changes  made in viterbi function) and upper case already done(in predict function) and K fold cross validation**"
      ],
      "metadata": {
        "id": "9jCSKhC8AHxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class POSTagger:\n",
        "\n",
        "    def __init__(self, train_data, test_data, tagset) -> None:\n",
        "\n",
        "        self.test_data = test_data\n",
        "        self.train_data = train_data\n",
        "        self.tagset = tagset\n",
        "        self.tag_count = self.getTagCount()\n",
        "        self.lexical_prob = self.lexical_probability()\n",
        "        self.transition_prob = self.transition_probability()\n",
        "        self.start_prob = self.start_probability()\n",
        "        self.end_prob = self.end_probability()\n",
        "\n",
        "    def getTagCount(self):\n",
        "        tag_count = defaultdict(int)\n",
        "        for sent in self.train_data:\n",
        "            for (w, t) in sent:\n",
        "                tag_count[t] += 1\n",
        "        return tag_count\n",
        "\n",
        "    def lexical_probability(self):\n",
        "        lexical_prob = defaultdict(lambda: defaultdict(float))\n",
        "        for s in self.train_data:\n",
        "            for (w, t) in s:\n",
        "                lexical_prob[w.lower()][t] += 1\n",
        "        return lexical_prob\n",
        "\n",
        "    def transition_probability(self):\n",
        "        transition_prob = defaultdict(lambda: defaultdict(float))\n",
        "        for sent in self.train_data:\n",
        "            for (w1, t1), (w2, t2) in nltk.bigrams(sent):\n",
        "                transition_prob[t2][t1] += 1\n",
        "        for t2 in self.tagset:\n",
        "            for t1 in self.tagset:\n",
        "                transition_prob[t2][t1] /= self.tag_count[t1]\n",
        "        return transition_prob\n",
        "\n",
        "    def start_probability(self):\n",
        "        start_prob = defaultdict(float)\n",
        "        for sent in self.train_data:\n",
        "            start_prob[sent[0][1]] += 1\n",
        "        for t in self.tagset:\n",
        "            start_prob[t] /= len(self.train_data)\n",
        "        return start_prob\n",
        "\n",
        "    def end_probability(self):\n",
        "        end_prob = defaultdict(float)\n",
        "        for sent in self.train_data:\n",
        "            end_prob[sent[len(sent)-1][1]] += 1\n",
        "        for t in self.tagset:\n",
        "            end_prob[t] /= len(self.train_data)\n",
        "        return end_prob\n",
        "\n",
        "    def viterbi(self, words):\n",
        "        dp = [defaultdict(float) for _ in range(len(words) + 1)]\n",
        "        backpointers = [defaultdict(int) for _ in range(len(words)+1)]\n",
        "\n",
        "        # Initialize for the first word\n",
        "        for t in self.tagset:\n",
        "            dp[0][t] = self.start_prob[t] * self.lexical_prob[words[0]][t] / self.tag_count[t] if words[0] in self.lexical_prob else self.start_prob[t] * 0.1  # Default probability for unknown words\n",
        "\n",
        "        for i in range(1, len(words)):\n",
        "            for t in self.tagset:\n",
        "                if words[i] in self.lexical_prob:\n",
        "                    dp[i][t], backpointers[i][t] = max((dp[i-1][prev_t] * self.transition_prob[t][prev_t] * self.lexical_prob[words[i]][t] / self.tag_count[t], prev_t) for prev_t in self.tagset)\n",
        "                else:\n",
        "                    # Handling unknown words\n",
        "                    dp[i][t], backpointers[i][t] = max((dp[i-1][prev_t] * self.transition_prob[t][prev_t] * 0.1, prev_t) for prev_t in self.tagset)  # Default probability for unknown words\n",
        "\n",
        "        dp[len(words)]['.'], backpointers[len(words)]['.'] = max((dp[len(words)-1][prev_t] * self.end_prob[prev_t], prev_t) for prev_t in self.tagset)\n",
        "        best_path = [backpointers[len(words)]['.']]\n",
        "\n",
        "        for i in range(len(words)-1, 0, -1):\n",
        "            best_path.append(backpointers[i][best_path[-1]])\n",
        "\n",
        "        best_path.reverse()\n",
        "\n",
        "        return best_path\n",
        "\n",
        "    def evaluate(self):\n",
        "      true_tags = []\n",
        "      predicted_tags = []\n",
        "\n",
        "      for sentence in self.test_data:\n",
        "        words = [w for w, t in sentence]\n",
        "        true_tags_sentence = [t for w, t in sentence]\n",
        "        predicted_tags_sentence = self.viterbi(words)\n",
        "\n",
        "        true_tags.extend(true_tags_sentence)\n",
        "        predicted_tags.extend(predicted_tags_sentence)\n",
        "\n",
        "    # Calculate overall metrics\n",
        "      overall_precision = precision_score(true_tags, predicted_tags, average='weighted', zero_division=0)\n",
        "      overall_recall = recall_score(true_tags, predicted_tags, average='weighted', zero_division=0)\n",
        "      overall_f1 = f1_score(true_tags, predicted_tags, average='weighted', zero_division=0)\n",
        "\n",
        "    # Confusion matrix\n",
        "      conf_matrix = confusion_matrix(true_tags, predicted_tags, labels=self.tagset)\n",
        "\n",
        "    # Print overall metrics\n",
        "      print(\"Overall Precision:\", overall_precision)\n",
        "      print(\"Overall Recall:\", overall_recall)\n",
        "      print(\"Overall F1 Score:\", overall_f1)\n",
        "      print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "    # Calculate metrics for individual tags\n",
        "      print(\"\\nMetrics for Individual Tags:\")\n",
        "      report = classification_report(true_tags, predicted_tags, labels=self.tagset)\n",
        "      print(report)\n",
        "\n",
        "    # Optionally you can add your test method here (omitted for brevity)\n"
      ],
      "metadata": {
        "id": "sRt8C8iI_qQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we includes K-fold cross validation"
      ],
      "metadata": {
        "id": "amVkh4iZNNPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    data = brown.tagged_sents(tagset='universal')\n",
        "    tagset = ['PRT', 'ADP', 'DET', 'ADV', 'PRON', '.', 'NUM', 'NOUN', 'ADJ', 'X', 'VERB', 'CONJ']\n",
        "\n",
        "    # K-Fold Cross-Validation\n",
        "    kf = KFold(n_splits=5)\n",
        "    for train_index, test_index in kf.split(data):\n",
        "        train_data = [data[i] for i in train_index]\n",
        "        test_data = [data[i] for i in test_index]\n",
        "\n",
        "        # Initialize and evaluate POSTagger\n",
        "        tagger = POSTagger(train_data, test_data, tagset)\n",
        "        tagger.evaluate()\n",
        "\n",
        "    # Function to predict POS tags for Gradio\n",
        "    def predict(sentence):\n",
        "        words = [word.lower() for word in nltk.word_tokenize(sentence)]\n",
        "        pos_tags = tagger.viterbi(words)\n",
        "        return list(zip(words, pos_tags))  # Pair words with their POS tags\n",
        "\n",
        "    # Gradio Interface\n",
        "    interface = gr.Interface(\n",
        "        fn=predict,\n",
        "        inputs=\"text\",\n",
        "        outputs=\"text\",\n",
        "        title=\"POS Tagger\",\n",
        "        description=\"Input a sentence to get POS tags.\"\n",
        "    )\n",
        "    interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d0xE3oJsCzwF",
        "outputId": "fbdc5044-7a46-4fbd-bf10-f05232b97729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Precision: 0.8937750864531743\n",
            "Overall Recall: 0.8872184054960857\n",
            "Overall F1 Score: 0.8869386333483691\n",
            "Confusion Matrix:\n",
            " [[ 4765   558    52    46   252     3     0    69    17     1    28     0]\n",
            " [  432 29459   445   347   217     6     0    72    13     4    21    32]\n",
            " [    1   448 28182    47   668     1     0    91     2     4     8     3]\n",
            " [   87   596   148  8964   174    13     0   489   449     1    40    10]\n",
            " [    0   131    98    66  7662     2     0    99     0     0   146     0]\n",
            " [    0     0     0     0     0 30110     0     0     0     5     0     0]\n",
            " [    0    39   200     2    27     7  3309   322    48     1    19     0]\n",
            " [    6  2227  4950    68  1681   489    48 52995  2181    23  2122     0]\n",
            " [   28   298  1411   489    69    46     4  1231 15184     5   176     0]\n",
            " [    0    23    48     0     2     3     0   106    13    81     5     0]\n",
            " [    1   431   309    54    60    61     0  1579   257     6 34370     0]\n",
            " [    0   239    75    74    94     2     0   129     2     2     2  7043]]\n",
            "\n",
            "Metrics for Individual Tags:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         PRT       0.90      0.82      0.86      5791\n",
            "         ADP       0.86      0.95      0.90     31048\n",
            "         DET       0.78      0.96      0.86     29455\n",
            "         ADV       0.88      0.82      0.85     10971\n",
            "        PRON       0.70      0.93      0.80      8204\n",
            "           .       0.98      1.00      0.99     30115\n",
            "         NUM       0.98      0.83      0.90      3974\n",
            "        NOUN       0.93      0.79      0.85     66790\n",
            "         ADJ       0.84      0.80      0.82     18941\n",
            "           X       0.61      0.29      0.39       281\n",
            "        VERB       0.93      0.93      0.93     37128\n",
            "        CONJ       0.99      0.92      0.95      7662\n",
            "\n",
            "    accuracy                           0.89    250360\n",
            "   macro avg       0.87      0.84      0.84    250360\n",
            "weighted avg       0.89      0.89      0.89    250360\n",
            "\n",
            "Overall Precision: 0.910745836745194\n",
            "Overall Recall: 0.9074769957122898\n",
            "Overall F1 Score: 0.9073727030548784\n",
            "Confusion Matrix:\n",
            " [[ 4705   547    53    21   283     0     0    61     6     1     8     0]\n",
            " [  470 30437   508   429   275     6     1    87    15    10     9    28]\n",
            " [    0   298 29333     5   651     2     0    75     2     8     6     4]\n",
            " [  105   765   203  9501   274    15     0   537   546     1    63    19]\n",
            " [   24   121    74     0  8143     0     0    85     0     0   139     0]\n",
            " [    0     0     0     0     0 29722     0     0     0    15     0     0]\n",
            " [    0    20   143     0    29    10  2549   347     8     0    10     0]\n",
            " [   51   994  2510    28  1214   308    61 52599  1314    26  1578     0]\n",
            " [   52   161  1258   514    65    60     0  1164 16236     5   169     0]\n",
            " [    1    14    29     0     3     3     0   112    15    83     4     0]\n",
            " [    0   617   350    20   151    62     0  1902   251     8 34639     0]\n",
            " [    0   194    73    17   154     5     0    98     3     2     4  8091]]\n",
            "\n",
            "Metrics for Individual Tags:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         PRT       0.87      0.83      0.85      5685\n",
            "         ADP       0.89      0.94      0.92     32275\n",
            "         DET       0.85      0.97      0.90     30384\n",
            "         ADV       0.90      0.79      0.84     12029\n",
            "        PRON       0.72      0.95      0.82      8586\n",
            "           .       0.98      1.00      0.99     29737\n",
            "         NUM       0.98      0.82      0.89      3116\n",
            "        NOUN       0.92      0.87      0.89     60683\n",
            "         ADJ       0.88      0.82      0.85     19684\n",
            "           X       0.52      0.31      0.39       264\n",
            "        VERB       0.95      0.91      0.93     38000\n",
            "        CONJ       0.99      0.94      0.96      8641\n",
            "\n",
            "    accuracy                           0.91    249084\n",
            "   macro avg       0.87      0.85      0.85    249084\n",
            "weighted avg       0.91      0.91      0.91    249084\n",
            "\n",
            "Overall Precision: 0.9129490297843217\n",
            "Overall Recall: 0.9090380246082473\n",
            "Overall F1 Score: 0.9091825745233302\n",
            "Confusion Matrix:\n",
            " [[ 4881   405    39    28   258     1     0    50     4     4    15     0]\n",
            " [  508 36257   693   418   420    26     0   102    12    29    51    40]\n",
            " [    0   367 31652    12   717     1     0   110    10    19     8     3]\n",
            " [   61   657   168  9772   230     9     0   621   509     4    59    15]\n",
            " [    0   124    70     0  7591     2     0   114     1     1   186     0]\n",
            " [    0     1     1     0     0 30684     0     2     0    32     0     0]\n",
            " [    0    26   226     0    36     4  3836   336    17     4    13     0]\n",
            " [    9  1181  2932    15  1602   416    80 58749  1515    70  1901     0]\n",
            " [   46   196  1533   464    60    44     0  1306 18131    21   202     0]\n",
            " [    2    45    59     3    13     4     0   124    23   134    34     4]\n",
            " [    1   310   267    19    77    61     0  1556   305    26 37913     0]\n",
            " [    0   163    56    20   102     1     0    96     3     8     8  8861]]\n",
            "\n",
            "Metrics for Individual Tags:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         PRT       0.89      0.86      0.87      5685\n",
            "         ADP       0.91      0.94      0.93     38556\n",
            "         DET       0.84      0.96      0.90     32899\n",
            "         ADV       0.91      0.81      0.86     12105\n",
            "        PRON       0.68      0.94      0.79      8089\n",
            "           .       0.98      1.00      0.99     30720\n",
            "         NUM       0.98      0.85      0.91      4498\n",
            "        NOUN       0.93      0.86      0.89     68470\n",
            "         ADJ       0.88      0.82      0.85     22003\n",
            "           X       0.38      0.30      0.34       445\n",
            "        VERB       0.94      0.94      0.94     40535\n",
            "        CONJ       0.99      0.95      0.97      9318\n",
            "\n",
            "    accuracy                           0.91    273323\n",
            "   macro avg       0.86      0.85      0.85    273323\n",
            "weighted avg       0.91      0.91      0.91    273323\n",
            "\n",
            "Overall Precision: 0.9138072709269961\n",
            "Overall Recall: 0.9115183271380856\n",
            "Overall F1 Score: 0.9110868691065364\n",
            "Confusion Matrix:\n",
            " [[ 4777   533    69    63   422     7     0   195     6     2    81     0]\n",
            " [  380 23141   323   322   139    11     0    57    13     7    37    23]\n",
            " [    0   290 23334    28   538     1     0    47     1     5     4     3]\n",
            " [  195   639   205  8541   169    11     0   586   384     5    91     9]\n",
            " [   44    69   123    66 10035     2     0   164     7     1   160     0]\n",
            " [    0     0     0     0     0 27905     0     0     0     5     0     0]\n",
            " [    0    15    98     1     8     3  1890   182     5     2    10     0]\n",
            " [   73   549  2116    61  1535   165    48 38968   916    11  1142     1]\n",
            " [   33    98   689   376    19    18     0   625 11194     6   180     0]\n",
            " [    1    21    23     0     2    25     5    83    12    39     4     0]\n",
            " [    5   401   204    24    76   101     0  1209   140    11 32625     0]\n",
            " [    0   148    68    78    61     7     0    61     1     3     4  6125]]\n",
            "\n",
            "Metrics for Individual Tags:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         PRT       0.87      0.78      0.82      6155\n",
            "         ADP       0.89      0.95      0.92     24453\n",
            "         DET       0.86      0.96      0.91     24251\n",
            "         ADV       0.89      0.79      0.84     10835\n",
            "        PRON       0.77      0.94      0.85     10671\n",
            "           .       0.99      1.00      0.99     27910\n",
            "         NUM       0.97      0.85      0.91      2214\n",
            "        NOUN       0.92      0.85      0.89     45585\n",
            "         ADJ       0.88      0.85      0.86     13238\n",
            "           X       0.40      0.18      0.25       215\n",
            "        VERB       0.95      0.94      0.94     34796\n",
            "        CONJ       0.99      0.93      0.96      6556\n",
            "\n",
            "    accuracy                           0.91    206879\n",
            "   macro avg       0.87      0.84      0.85    206879\n",
            "weighted avg       0.91      0.91      0.91    206879\n",
            "\n",
            "Overall Precision: 0.9038849831994047\n",
            "Overall Recall: 0.9028565762947132\n",
            "Overall F1 Score: 0.902105343393758\n",
            "Confusion Matrix:\n",
            " [[ 4618   663   154    78   471     0     0   380    10     2   137     0]\n",
            " [  464 17190   213   335    87     0     0    61    23     6    34    21]\n",
            " [    0   284 19138    48   497     0     2    50     0     4     4     3]\n",
            " [  245   583   202  7881   176     4     0   693   375     0   109    31]\n",
            " [   82    63   244   102 12514     6     0   442     2     0   329     0]\n",
            " [    0     0     0     0     0 29081     0     0     0     2     0     0]\n",
            " [    0     2    68     3     8     5   922    55     6     0     3     0]\n",
            " [   69   293  1392    61  1463    86    87 28863   675    20  1021     0]\n",
            " [   11    49   403   373    24     9     0   539  8332     2   113     0]\n",
            " [    1     7    19     0     4     5     0    90     9    40     6     0]\n",
            " [    1   614   169    63   103    20     0  1160   143     4 30014     0]\n",
            " [    0   274   106    88    76     0     0    89     0     1    23  5317]]\n",
            "\n",
            "Metrics for Individual Tags:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         PRT       0.84      0.71      0.77      6513\n",
            "         ADP       0.86      0.93      0.89     18434\n",
            "         DET       0.87      0.96      0.91     20030\n",
            "         ADV       0.87      0.77      0.82     10299\n",
            "        PRON       0.81      0.91      0.86     13784\n",
            "           .       1.00      1.00      1.00     29083\n",
            "         NUM       0.91      0.86      0.89      1072\n",
            "        NOUN       0.89      0.85      0.87     34030\n",
            "         ADJ       0.87      0.85      0.86      9855\n",
            "           X       0.49      0.22      0.31       181\n",
            "        VERB       0.94      0.93      0.94     32291\n",
            "        CONJ       0.99      0.89      0.94      5974\n",
            "\n",
            "    accuracy                           0.90    181546\n",
            "   macro avg       0.86      0.82      0.84    181546\n",
            "weighted avg       0.90      0.90      0.90    181546\n",
            "\n",
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://1b9ddaf8d88a7a7807.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1b9ddaf8d88a7a7807.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}