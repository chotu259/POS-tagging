{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h08TmK-6pkE",
        "outputId": "310246e0-5ac3-4048-d916-c7be91614915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (1.5.2)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (4.66.5)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (3.5.0)\n",
            "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.10 sklearn-crfsuite-0.5.0\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0 (from gradio)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m832.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1.0->gradio)\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading ruff-0.6.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, h11, ffmpy, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.0 ffmpy-0.4.0 gradio-4.44.1 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.12 ruff-0.6.8 semantic-version-2.10.0 starlette-0.38.6 tomlkit-0.12.0 uvicorn-0.31.0 websockets-12.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn-crfsuite\n",
        "!pip install gradio\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1GdC5ESIH3Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T58RcbcxH3hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')  # Optional, if tokenizing\n",
        "nltk.download('universal_tagset')  # For Universal POS tagging\n",
        "\n",
        "from nltk import bigrams\n",
        "from collections import defaultdict\n",
        "from sklearn_crfsuite import CRF, metrics\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "class CRFTagger:\n",
        "    def __init__(self, train_data, test_data, tagset) -> None:\n",
        "        self.train_data = train_data\n",
        "        self.test_data = test_data\n",
        "        self.tagset = tagset\n",
        "\n",
        "        self.train_features = self.extract_features(self.train_data)\n",
        "        self.test_features = self.extract_features(self.test_data)\n",
        "\n",
        "        self.crf = CRF(algorithm='lbfgs',c1=0.01,c2=0.1,max_iterations=100,all_possible_transitions=True)\n",
        "        self.crf.fit(self.train_features, self.get_labels(self.train_data))\n",
        "\n",
        "    def extract_features(self, data):\n",
        "      features = []\n",
        "      for sentence in data:\n",
        "          sentence_features = []\n",
        "          for i, (word, tag) in enumerate(sentence):\n",
        "              word_features = {\n",
        "                  'word': word.lower(),\n",
        "                  'is_upper': word.isupper(),\n",
        "                  'is_title': word.istitle(),\n",
        "                  'is_digit': word.isdigit(),\n",
        "                  'is_punct': word in \".,;:!?\",\n",
        "                  'contains_digit': any(char.isdigit() for char in word),\n",
        "                  'contains_hyphen': '-' in word,\n",
        "                  'word_len': len(word),\n",
        "\n",
        "                  # Prefixes and Suffixes\n",
        "                  'pref_1': word[:1],\n",
        "                  'pref_2': word[:2],\n",
        "                  'pref_3': word[:3],\n",
        "                  'pref_4': word[:4],\n",
        "                  'suff_1': word[-1:],\n",
        "                  'suff_2': word[-2:],\n",
        "                  'suff_3': word[-3:],\n",
        "                  'suff_4': word[-4:],\n",
        "\n",
        "                  # Previous word and next word context\n",
        "                  'prev_word': '' if i == 0 else sentence[i - 1][0].lower(),\n",
        "                  'prev2_word': '' if i <= 1 else sentence[i - 2][0].lower(),\n",
        "                  'next_word': '' if i == len(sentence) - 1 else sentence[i + 1][0].lower(),\n",
        "                  'next2_word': '' if i >= len(sentence) - 2 else sentence[i + 2][0].lower(),\n",
        "\n",
        "                  # POS tags of neighbors (only during training or if tags are available)\n",
        "                  'prev_pos': '' if i == 0 else sentence[i - 1][1],\n",
        "                  'prev2_pos': '' if i <= 1 else sentence[i - 2][1],\n",
        "                  'next_pos': '' if i == len(sentence) - 1 else sentence[i + 1][1],\n",
        "                  'next2_pos': '' if i >= len(sentence) - 2 else sentence[i + 2][1]\n",
        "              }\n",
        "              sentence_features.append(word_features)\n",
        "          features.append(sentence_features)\n",
        "      return features\n",
        "\n",
        "\n",
        "    def get_labels(self, data):\n",
        "        return [[tag for _, tag in sentence] for sentence in data]\n",
        "\n",
        "    def evaluate(self):\n",
        "        y_pred = self.crf.predict(self.test_features)\n",
        "        y_test = self.get_labels(self.test_data)\n",
        "\n",
        "        # Overall metrics\n",
        "        overall_accuracy = metrics.flat_accuracy_score(y_test, y_pred)\n",
        "        overall_precision = precision_score([tag for tags in y_test for tag in tags],\n",
        "                                            [tag for tags in y_pred for tag in tags],\n",
        "                                            average='weighted', zero_division=0)\n",
        "        overall_recall = recall_score([tag for tags in y_test for tag in tags],\n",
        "                                      [tag for tags in y_pred for tag in tags],\n",
        "                                      average='weighted', zero_division=0)\n",
        "        overall_f1 = f1_score([tag for tags in y_test for tag in tags],\n",
        "                              [tag for tags in y_pred for tag in tags],\n",
        "                              average='weighted', zero_division=0)\n",
        "\n",
        "        # Confusion matrix\n",
        "        conf_matrix = confusion_matrix([tag for tags in y_test for tag in tags],\n",
        "                                       [tag for tags in y_pred for tag in tags],\n",
        "                                       labels=self.tagset)\n",
        "\n",
        "        # Print overall metrics\n",
        "        print(\"Overall Accuracy:\", overall_accuracy)\n",
        "        print(\"Overall Precision:\", overall_precision)\n",
        "        print(\"Overall Recall:\", overall_recall)\n",
        "        print(\"Overall F1 Score:\", overall_f1)\n",
        "        print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "        # Individual tag metrics\n",
        "        print(\"\\nMetrics for Individual Tags:\")\n",
        "        report = classification_report([tag for tags in y_test for tag in tags],\n",
        "                                       [tag for tags in y_pred for tag in tags],\n",
        "                                       labels=self.tagset)\n",
        "        print(report)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNKrXlhd7Vrb",
        "outputId": "6accd31c-1790-4e32-f46b-e8b767632ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load data and initialize tagger\n",
        "    from nltk.corpus import brown\n",
        "    data = brown.tagged_sents(tagset='universal')\n",
        "    taglist=['NOUN', 'VERB', 'PRON', 'ADP', 'PRT', '.', 'CONJ', 'DET', 'ADJ', 'ADV', 'NUM', 'X']\n",
        "\n",
        "\n",
        "    # Preprocess data: tokenization and lowercasing\n",
        "    kf = KFold(n_splits=5)\n",
        "    for train_index, test_index in kf.split(data):\n",
        "        train_data = [data[i] for i in train_index]\n",
        "        test_data = [data[i] for i in test_index]\n",
        "\n",
        "        # Initialize and evaluate POSTagger\n",
        "        tagger = CRFTagger(train_data, test_data, taglist)\n",
        "        tagger.evaluate()\n",
        "\n",
        "    # Function to predict POS tags for Gradio\n",
        "    def predict(sentence):\n",
        "        words = [word.lower() for word in word_tokenize(sentence)]  # Ensure all words are lowercased\n",
        "        features = tagger.extract_features([[ (word, '') for word in words ]])  # Create dummy tags for prediction\n",
        "        pos_tags = tagger.crf.predict(features)[0]  # Predict using the CRF model\n",
        "        return list(zip(words, pos_tags))  # Pair words with their POS tags\n",
        "\n",
        "    # Gradio Interface\n",
        "    import gradio as gr\n",
        "    interface = gr.Interface(\n",
        "        fn=predict,\n",
        "        inputs=\"text\",\n",
        "        outputs=\"text\",\n",
        "        title=\"POS Tagger\",\n",
        "        description=\"Input a sentence to get POS tags.\"\n",
        "    )\n",
        "    interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gdkjkxF8CPP",
        "outputId": "1cc85ad9-b1e4-416c-ba0c-83c56959b659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy: 0.9787905416200671\n",
            "Overall Precision: 0.978771734625255\n",
            "Overall Recall: 0.9787905416200671\n",
            "Overall F1 Score: 0.9787538815745895\n",
            "Confusion Matrix:\n",
            " [[65230   555     8    10    22     1     0     2   862    53    30    17]\n",
            " [  665 36333     0    17     3     0     0     0    93    17     0     0]\n",
            " [    2     0  8123    36     1     0     0    42     0     0     0     0]\n",
            " [    2     8    18 30602   190     0    22    55     9   142     0     0]\n",
            " [   32    12     0   197  5499     0     0     0     7    44     0     0]\n",
            " [    0     0     0     2     0 30113     0     0     0     0     0     0]\n",
            " [    0     0     0     2     0     0  7644     3     0    13     0     0]\n",
            " [    2     1    44    64     0     0     6 29318     1    18     0     1]\n",
            " [  639   151     0    22     6     0     0     0 17764   346    12     1]\n",
            " [   58    11     1   205    61     0    20    19   259 10336     0     1]\n",
            " [   53     0     0     0     0     0     0     0     7     1  3912     1]\n",
            " [   91     5     0     0     0     1     0     2     6     0     0   176]]\n",
            "\n",
            "Metrics for Individual Tags:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        NOUN       0.98      0.98      0.98     66790\n",
            "        VERB       0.98      0.98      0.98     37128\n",
            "        PRON       0.99      0.99      0.99      8204\n",
            "         ADP       0.98      0.99      0.98     31048\n",
            "         PRT       0.95      0.95      0.95      5791\n",
            "           .       1.00      1.00      1.00     30115\n",
            "        CONJ       0.99      1.00      1.00      7662\n",
            "         DET       1.00      1.00      1.00     29455\n",
            "         ADJ       0.93      0.94      0.94     18941\n",
            "         ADV       0.94      0.94      0.94     10971\n",
            "         NUM       0.99      0.98      0.99      3974\n",
            "           X       0.89      0.63      0.74       281\n",
            "\n",
            "    accuracy                           0.98    250360\n",
            "   macro avg       0.97      0.95      0.96    250360\n",
            "weighted avg       0.98      0.98      0.98    250360\n",
            "\n",
            "Overall Accuracy: 0.978276404746993\n",
            "Overall Precision: 0.9782646182856848\n",
            "Overall Recall: 0.978276404746993\n",
            "Overall F1 Score: 0.9782312545814296\n",
            "Confusion Matrix:\n",
            " [[59337   535     6    13    16     0     0     5   678    60    29     4]\n",
            " [  884 36968     0    25     4     0     1     0    98    19     0     1]\n",
            " [    6     0  8496    46     4     0     0    34     0     0     0     0]\n",
            " [    9     9    16 31799   228     0    15    41    14   143     1     0]\n",
            " [   12     3     0   196  5427     0     0     0    12    35     0     0]\n",
            " [    0     0     0     0     0 29736     0     0     0     0     0     1]\n",
            " [    0     0     0     6     0     0  8606     7     0    22     0     0]\n",
            " [    3     1    25    35     0     0     4 30306     2     8     0     0]\n",
            " [  718   154     0    18     7     0     0     0 18483   296     5     3]\n",
            " [   58     6     0   263    37     0    14    34   333 11284     0     0]\n",
            " [   49     0     0     0     0     0     0     0     8     0  3059     0]\n",
            " [   86     2     0     0     0     0     0     0     3     1     0   172]]\n",
            "\n",
            "Metrics for Individual Tags:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        NOUN       0.97      0.98      0.97     60683\n",
            "        VERB       0.98      0.97      0.98     38000\n",
            "        PRON       0.99      0.99      0.99      8586\n",
            "         ADP       0.98      0.99      0.98     32275\n",
            "         PRT       0.95      0.95      0.95      5685\n",
            "           .       1.00      1.00      1.00     29737\n",
            "        CONJ       1.00      1.00      1.00      8641\n",
            "         DET       1.00      1.00      1.00     30384\n",
            "         ADJ       0.94      0.94      0.94     19684\n",
            "         ADV       0.95      0.94      0.94     12029\n",
            "         NUM       0.99      0.98      0.99      3116\n",
            "           X       0.95      0.65      0.77       264\n",
            "\n",
            "    accuracy                           0.98    249084\n",
            "   macro avg       0.97      0.95      0.96    249084\n",
            "weighted avg       0.98      0.98      0.98    249084\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AAkHlDyQIJYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3GRBkM0AIJ3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-u5UJi-pIKSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')  # Optional, if tokenizing\n",
        "nltk.download('universal_tagset')  # For Universal POS tagging\n",
        "\n",
        "from nltk import bigrams\n",
        "from collections import defaultdict\n",
        "from sklearn_crfsuite import CRF, metrics\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "class CRFTagger:\n",
        "    def __init__(self, train_data, test_data, tagset) -> None:\n",
        "        self.train_data = train_data\n",
        "        self.test_data = test_data\n",
        "        self.tagset = tagset\n",
        "\n",
        "        self.train_features = self.extract_features(self.train_data)\n",
        "        self.test_features = self.extract_features(self.test_data)\n",
        "\n",
        "        self.crf = CRF(algorithm='lbfgs',c1=0.01,c2=0.1,max_iterations=100,all_possible_transitions=True)\n",
        "        self.crf.fit(self.train_features, self.get_labels(self.train_data))\n",
        "\n",
        "    def extract_features(self, data):\n",
        "        features = []\n",
        "        for sent in data:\n",
        "            sentence_features = []\n",
        "            for i, (word, tag) in enumerate(sent):\n",
        "                word=sent[i][0]\n",
        "                pos=sent[i][1]\n",
        "                if(i==0):\n",
        "                    prevw='<START>'\n",
        "                    prepos='<START>'\n",
        "                else:\n",
        "                    prevw=sent[i-1][0]\n",
        "                    prepos=sent[i-1][1]\n",
        "                if(i==0 or i==1):\n",
        "                     prev2='<START>'\n",
        "                     prev2pos='<START>'\n",
        "                else:\n",
        "                    prev2=sent[i-2][0]\n",
        "                    prev2pos=sent[i-2][1]\n",
        "                if(i==len(sent)-1):\n",
        "                    nextw='<END>'\n",
        "                    nextpos='<END>'\n",
        "                else:\n",
        "                    nextw=sent[i+1][0]\n",
        "                    nextpos=sent[i+1][1]\n",
        "                pref_1, pref_2, pref_3, pref_4=word[:1],word[:2],word[:3],word[:4]\n",
        "                suff_1, suff_2, suff_3, suff_4=word[-1:],word[-2:],word[-3:],word[-4:]\n",
        "                word_features = {\n",
        "                    'word': word.lower(),  # Normalize to lowercase\n",
        "                    'is_upper': word.isupper(),\n",
        "                    'is_title': word.istitle(),\n",
        "                    'is_digit': word.isdigit(),\n",
        "                    'pos':pos,\n",
        "                    'prevword':prevw,\n",
        "                    'prevpos':prepos,\n",
        "                    'prev2word':prev2,\n",
        "                    'prev2pos':prev2pos,\n",
        "                    'nextword': nextw,\n",
        "                    'nextpos': nextpos,\n",
        "                    'pref_1':word[:1],\n",
        "                    'pref_2':word[:2],\n",
        "                    'pref_3':word[:3],\n",
        "                    'pref_4':word[:4],\n",
        "                    'suff_1':word[-1:],\n",
        "                    'suff_2':word[-2:],\n",
        "                    'suff_3':word[-3:],\n",
        "                    'suff_4':word[-4:]\n",
        "                }\n",
        "            sentence_features.append(word_features)\n",
        "        features.append(sentence_features)\n",
        "        return features\n",
        "\n",
        "    def get_labels(self, data):\n",
        "        return [[tag for _, tag in sentence] for sentence in data]\n",
        "\n",
        "    def evaluate(self):\n",
        "        y_pred = self.crf.predict(self.test_features)\n",
        "        y_test = self.get_labels(self.test_data)\n",
        "\n",
        "        # Overall metrics\n",
        "        overall_accuracy = metrics.flat_accuracy_score(y_test, y_pred)\n",
        "        overall_precision = precision_score([tag for tags in y_test for tag in tags],\n",
        "                                            [tag for tags in y_pred for tag in tags],\n",
        "                                            average='weighted', zero_division=0)\n",
        "        overall_recall = recall_score([tag for tags in y_test for tag in tags],\n",
        "                                      [tag for tags in y_pred for tag in tags],\n",
        "                                      average='weighted', zero_division=0)\n",
        "        overall_f1 = f1_score([tag for tags in y_test for tag in tags],\n",
        "                              [tag for tags in y_pred for tag in tags],\n",
        "                              average='weighted', zero_division=0)\n",
        "\n",
        "        # Confusion matrix\n",
        "        conf_matrix = confusion_matrix([tag for tags in y_test for tag in tags],\n",
        "                                       [tag for tags in y_pred for tag in tags],\n",
        "                                       labels=self.tagset)\n",
        "\n",
        "        # Print overall metrics\n",
        "        print(\"Overall Accuracy:\", overall_accuracy)\n",
        "        print(\"Overall Precision:\", overall_precision)\n",
        "        print(\"Overall Recall:\", overall_recall)\n",
        "        print(\"Overall F1 Score:\", overall_f1)\n",
        "        print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "        # Individual tag metrics\n",
        "        print(\"\\nMetrics for Individual Tags:\")\n",
        "        report = classification_report([tag for tags in y_test for tag in tags],\n",
        "                                       [tag for tags in y_pred for tag in tags],\n",
        "                                       labels=self.tagset)\n",
        "        print(report)\n",
        "\n"
      ],
      "metadata": {
        "id": "FTSGpDU46sb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load data and initialize tagger\n",
        "    from nltk.corpus import brown\n",
        "    data = brown.tagged_sents(tagset='universal')\n",
        "    taglist=['NOUN', 'VERB', 'PRON', 'ADP', 'PRT', '.', 'CONJ', 'DET', 'ADJ', 'ADV', 'NUM', 'X']\n",
        "\n",
        "\n",
        "    # Preprocess data: tokenization and lowercasing\n",
        "    kf = KFold(n_splits=5)\n",
        "    for train_index, test_index in kf.split(data):\n",
        "        train_data = [data[i] for i in train_index]\n",
        "        test_data = [data[i] for i in test_index]\n",
        "\n",
        "        # Initialize and evaluate POSTagger\n",
        "        tagger = CRFTagger(train_data, test_data, taglist)\n",
        "        tagger.evaluate()\n",
        "\n",
        "    # Function to predict POS tags for Gradio\n",
        "    def predict(sentence):\n",
        "        words = [word.lower() for word in word_tokenize(sentence)]  # Ensure all words are lowercased\n",
        "        features = tagger.extract_features([[ (word, '') for word in words ]])  # Create dummy tags for prediction\n",
        "        pos_tags = tagger.crf.predict(features)[0]  # Predict using the CRF model\n",
        "        return list(zip(words, pos_tags))  # Pair words with their POS tags\n",
        "\n",
        "    # Gradio Interface\n",
        "    import gradio as gr\n",
        "    interface = gr.Interface(\n",
        "        fn=predict,\n",
        "        inputs=\"text\",\n",
        "        outputs=\"text\",\n",
        "        title=\"POS Tagger\",\n",
        "        description=\"Input a sentence to get POS tags.\"\n",
        "    )\n",
        "    interface.launch()"
      ],
      "metadata": {
        "id": "_6adGspV6vO7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}